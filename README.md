<div align="center">

# StructureFlow: Simulation-free Structure Learningfor Stochastic Dynamics

<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
<a href="https://pytorchlightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white"></a>
<a href="https://hydra.cc/"><img alt="Config: Hydra" src="https://img.shields.io/badge/Config-Hydra-89b8cd"></a>
<a href="https://github.com/ashleve/lightning-hydra-template"><img alt="Template" src="https://img.shields.io/badge/-Lightning--Hydra--Template-017F2F?style=flat&logo=github&labelColor=gray"></a><br>
[![Paper](http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg)](https://www.nature.com/articles/nature14539)
[![Conference](http://img.shields.io/badge/AnyConference-year-4b44ce.svg)](https://papers.nips.cc/paper/2020)

</div>

## Description

What it does

## Installation

#### Pip

```bash
# clone project
git clone https://github.com/YourGithubName/your-repo-name
cd your-repo-name

# [OPTIONAL] create conda environment
conda create -n myenv python=3.9
conda activate myenv

# install pytorch according to instructions
# https://pytorch.org/get-started/

# install requirements
pip install -r requirements.txt
```

#### Conda

```bash
# clone project
git clone https://github.com/YourGithubName/your-repo-name
cd your-repo-name

# create conda environment and install dependencies
conda env create -f environment.yaml -n myenv

# activate conda environment
conda activate myenv
```

## Project Structure

The directory structure of new project looks like this:

```
├── .github                   <- Github Actions workflows
│
├── configs                   <- Hydra configs
│   ├── callbacks                <- Callbacks configs
│   ├── data                     <- Data configs
│   ├── debug                    <- Debugging configs
│   ├── experiment               <- Experiment configs
│   ├── extras                   <- Extra utilities configs
│   ├── hparams_search           <- Hyperparameter search configs
│   ├── hydra                    <- Hydra configs
│   ├── local                    <- Local configs
│   ├── logger                   <- Logger configs
│   ├── model                    <- Model configs
│   ├── paths                    <- Project paths configs
│   ├── trainer                  <- Trainer configs
│   │
│   ├── eval.yaml             <- Main config for evaluation
│   └── train.yaml            <- Main config for training
│
├── data                   <- Project data
│
├── logs                   <- Logs generated by hydra and lightning loggers
│
├── notebooks              <- Jupyter notebooks. Naming convention is a number (for ordering),
│                             the creator's initials, and a short `-` delimited description,
│                             e.g. `1.0-jqp-initial-data-exploration.ipynb`.
│
├── scripts                <- Shell scripts
│
├── src                    <- Source code
│   ├── data                     <- Data scripts
│   ├── models                   <- Model scripts
│   ├── utils                    <- Utility scripts
│   │
│   ├── eval.py                  <- Run evaluation
│   └── train.py                 <- Run training
│
├── tests                  <- Tests of any kind
|
├── tools                  <- Tooling to run other baselines
|   ├── SINCERITIES               <- Run SINCERITIES
│
├── .env.example              <- Example of file for storing private environment variables
├── .gitignore                <- List of files ignored by git
├── .pre-commit-config.yaml   <- Configuration of pre-commit hooks for code formatting
├── .project-root             <- File for inferring the position of project root directory
├── environment.yaml          <- File for installing conda environment
├── Makefile                  <- Makefile with commands like `make train` or `make test`
├── pyproject.toml            <- Configuration options for testing and linting
├── requirements.txt          <- File for installing python dependencies
├── setup.py                  <- File for installing project as a package
└── README.md
```

<br>

## How to run

Train model with default configuration

```bash
# train on CPU
python src/train.py trainer=cpu

# train on GPU
python src/train.py trainer=gpu
```

Train model with chosen experiment configuration from [configs/experiment/](configs/experiment/)

```bash
python src/train.py experiment=experiment_name.yaml
```

You can override any parameter from command line like this

```bash
python src/train.py trainer.max_epochs=20 data.batch_size=64
```

## Running Experiments

### Trajectory Inference Experiments

Run leave-one-timepoint-out trajectory inference experiments across multiple datasets:

```bash
python3 trajectory_inference_experiments.py
```

Optional arguments:
- `--base_results_dir`: Directory to save results (default: `loo_results`)
- `--only_aggregate`: Skip running experiments and only aggregate existing results
- `--num_workers`: Number of parallel workers (default: 1)
- `--sequential`: Run experiments sequentially instead of in parallel

Example with parallel execution:
```bash
python3 trajectory_inference_experiments.py --num_workers 4 --base_results_dir my_results
```

The script will:
- Run experiments on Synthetic (dyn-TF, dyn-CY, dyn-LL, dyn-BF, dyn-SW), Curated, and Renge datasets
- Test each dataset with multiple random seeds
- Aggregate results across seeds
- Generate comparison tables by timepoint and overall performance metrics

### Knockout Inference Experiments

Run leave-knockout-out inference experiments to test perturbation predictions:

```bash
python3 ko_inference_experiments.py
```

Optional arguments:
- `--base_results_dir`: Directory to save results (default: `lko_results`)
- `--only_aggregate`: Skip running experiments and only aggregate existing results
- `--num_workers`: Number of parallel workers (default: 1)
- `--sequential`: Run experiments sequentially instead of in parallel

Example:
```bash
python3 ko_inference_experiments.py --num_workers 2 --base_results_dir knockout_results
```

### Scaling Experiments

Run causal discovery experiments across different system sizes and sparsity levels:

```bash
cd scaling_experiment
python3 scaling_experiment.py
```

The scaling experiment:
- Tests causal discovery performance on systems ranging from 10 to 500 variables
- Evaluates multiple sparsity levels (0.05, 0.2, 0.4)
- Compares multiple methods: StructureFlow (SF2M), NGM-NODE, Reference Fitting (RF), and Correlation
- Uses sparsity-aware hyperparameter adjustments
- Generates AUROC and AUPRC metrics for causal graph recovery

You can customize the methods to run by editing the `main()` call at the bottom of the script:
```python
main(methods_to_run=["correlation", "rf", "sf2m", "ngm-node"])
```

### Baseline Comparisons

#### OTVelo Baseline

Run the OTVelo baseline on synthetic and curated datasets:

```bash
cd tools/OTVelo
bash otvelo_runner.sh
```

Environment variables (optional):
- `PYTHON`: Python executable to use (default: `python3`)
- `SCRIPT`: Script to run (default: `otvelo_baseline.py`)
- `DATA_ROOT`: Root directory for synthetic data (default: `../../data/Synthetic`)
- `HSC_PATH`: Path to curated data (default: `../../data/Curated`)

Example with custom paths:
```bash
DATA_ROOT=/path/to/data PYTHON=python3 bash otvelo_runner.sh
```

#### TIGON Baseline

Run the TIGON baseline on synthetic and curated datasets:

```bash
cd tools/TIGON
bash tigon_runner.sh
```

Environment variables (optional):
- `PYTHON`: Python executable to use (default: `python3`)
- `SCRIPT`: Script to run (default: `tigon_baseline.py`)
- `DATA_ROOT`: Root directory for synthetic data (default: `../../data/Synthetic`)
- `HSC_PATH`: Path to curated data (default: `../../data/Curated`)

Both baseline runners:
- Run experiments across multiple seeds (1, 2, 3)
- Process synthetic datasets (dyn-BF, dyn-TF, dyn-SW, dyn-CY, dyn-LL)
- Include HSC curated dataset if available
- Automatically aggregate results across seeds
- Report any failed runs

## License

Lightning-Hydra-Template is licensed under the MIT License.

```
Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```
